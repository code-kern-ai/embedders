{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5252aaca",
   "metadata": {},
   "source": [
    "# Finding similar sentences within a text corpus\n",
    "\n",
    "One great use case for embedding data is to use their vector representation for similarity search (often referred to as \"neural search\"). In this very short example, we'll show you how to build a super simple sentence comparison via comparing the cosine similarity of two embeddings.\n",
    "\n",
    "![A huge pile of embedded data points in vector space](https://miro.medium.com/max/2028/1*1LHBbqmPI0X4I3rio5ujWQ.png)\n",
    "\n",
    "This notebook is only meant as a tutorial; be aware that there are many fascinating neural search engines, such as [qdrant](https://qdrant.tech).\n",
    "\n",
    "---\n",
    "\n",
    "To get started, we just load two things: a function to load some sample data from the library, and a pre-trained sentence embedder based on transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedders.samples.clickbait import get_sample_data\n",
    "from embedders.classification.contextual import TransformerSentenceEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb52d5",
   "metadata": {},
   "source": [
    "The `clickbait` dataset is straightforward simple and consists of some short headliners. Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2559b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_sample_data()\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551608e",
   "metadata": {},
   "source": [
    "Next, we just load the embedder via some Hugging Face configuration string. We make use of `distilbert-base-uncased`. You can input any other model from the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TransformerSentenceEmbedder(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35567368",
   "metadata": {},
   "source": [
    "And now the magic happens: we encode the data. This is as easy as with your favorite sklearn objects - just call `fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedder.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822796f",
   "metadata": {},
   "source": [
    "Now, to compute a vanilla similarity search, we'll make use of the cosine similarity, which helps us to compute similarities for given vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector_1, vector_2):\n",
    "    return np.dot(vector_1, vector_2)/(np.linalg.norm(vector_1)*np.linalg.norm(vector_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678a387",
   "metadata": {},
   "source": [
    "And finally, a simplistic nested loop to calculate pairwise similarities (excluding identical sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d28194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "highest_similarity = float(\"-inf\")\n",
    "vector_pair = None, None\n",
    "for vector_1_idx, vector_1 in tqdm(enumerate(embeddings), total=len(embeddings)):\n",
    "    for vector_2_idx, vector_2 in enumerate(embeddings):\n",
    "        if vector_1_idx != vector_2_idx:\n",
    "            similarity = cosine_similarity(vector_1, vector_2)\n",
    "            if similarity > highest_similarity:\n",
    "                highest_similarity = similarity\n",
    "                vector_pair = vector_1_idx, vector_2_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e41ce",
   "metadata": {},
   "source": [
    "We can now take a look at the most similar pair in our text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[vector_pair[0]], texts[vector_pair[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd968e84",
   "metadata": {},
   "source": [
    "Wow - isn't that amazing?! Embedding data is one of the most sophisticated and intelligent way to enrich your records with valuable semantic metadata. There are sheer endless use cases. And `embedders` helps you to quickly generate embeddings for your dataset! üòã\n",
    "\n",
    "---\n",
    "\n",
    "If you have further questions, don't hesitate to contact us. If there is anything you want to have added to the library, open an [issue](https://github.com/code-kern-ai/embedders/issues). And please, don't forget to give us a ‚≠ê"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
